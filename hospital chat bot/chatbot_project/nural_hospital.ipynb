{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.0554 - loss: 3.0009 - val_accuracy: 0.0000e+00 - val_loss: 3.0686\n",
      "Epoch 2/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0704 - loss: 2.9567 - val_accuracy: 0.0000e+00 - val_loss: 3.1189\n",
      "Epoch 3/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0191 - loss: 2.9663 - val_accuracy: 0.0000e+00 - val_loss: 3.1707\n",
      "Epoch 4/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1770 - loss: 2.8834 - val_accuracy: 0.0000e+00 - val_loss: 3.2271\n",
      "Epoch 5/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1002 - loss: 2.8886 - val_accuracy: 0.0000e+00 - val_loss: 3.2895\n",
      "Epoch 6/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1615 - loss: 2.8412 - val_accuracy: 0.0000e+00 - val_loss: 3.3685\n",
      "Epoch 7/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1306 - loss: 2.8159 - val_accuracy: 0.0000e+00 - val_loss: 3.4665\n",
      "Epoch 8/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1680 - loss: 2.7998 - val_accuracy: 0.0000e+00 - val_loss: 3.5694\n",
      "Epoch 9/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1048 - loss: 2.8338 - val_accuracy: 0.0000e+00 - val_loss: 3.6714\n",
      "Epoch 10/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1005 - loss: 2.8116 - val_accuracy: 0.0000e+00 - val_loss: 3.7788\n",
      "Epoch 11/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1551 - loss: 2.7223 - val_accuracy: 0.0000e+00 - val_loss: 3.9063\n",
      "Epoch 12/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1396 - loss: 2.7127 - val_accuracy: 0.0000e+00 - val_loss: 4.0594\n",
      "Epoch 13/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1939 - loss: 2.6617 - val_accuracy: 0.0000e+00 - val_loss: 4.2335\n",
      "Epoch 14/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1116 - loss: 2.6762 - val_accuracy: 0.0000e+00 - val_loss: 4.3924\n",
      "Epoch 15/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1551 - loss: 2.6798 - val_accuracy: 0.0000e+00 - val_loss: 4.5098\n",
      "Epoch 16/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0970 - loss: 2.6650 - val_accuracy: 0.0000e+00 - val_loss: 4.6058\n",
      "Epoch 17/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0948 - loss: 2.6695 - val_accuracy: 0.0000e+00 - val_loss: 4.7033\n",
      "Epoch 18/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2502 - loss: 2.6177 - val_accuracy: 0.0000e+00 - val_loss: 4.8184\n",
      "Epoch 19/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2337 - loss: 2.5323 - val_accuracy: 0.0000e+00 - val_loss: 4.9384\n",
      "Epoch 20/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1712 - loss: 2.5844 - val_accuracy: 0.0000e+00 - val_loss: 5.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved as \"chatbot_model.h5\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Ensure NLTK data packages are available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Load the JSON data with error handling\n",
    "try:\n",
    "    with open('am_ill.json') as file:\n",
    "        data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: JSON file not found.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process illnesses and symptoms with data validation\n",
    "for illness in data['illnesses']:\n",
    "    if not isinstance(illness['symptoms'], list) or not isinstance(illness['illness'], str):\n",
    "        print(f\"Warning: Invalid data format for illness '{illness['illness']}'\")\n",
    "        continue\n",
    "    for symptom in illness['symptoms']:\n",
    "        # Tokenize and lemmatize the symptom text\n",
    "        words = word_tokenize(symptom)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "        X_train.append(\" \".join(lemmatized_words))\n",
    "        y_train.append(illness['illness'])\n",
    "\n",
    "# Check for empty training data before vectorization\n",
    "if not X_train:\n",
    "    print(\"Error: No training data for vectorization.\")\n",
    "else:\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_train_encoded = to_categorical(y_train_encoded)  # Convert to one-hot encoding\n",
    "\n",
    "    # Vectorize the symptoms using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "    # Define a more complex neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train_vectorized.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_vectorized, y_train_encoded, epochs=20, batch_size=8, validation_split=0.2)\n",
    "\n",
    "    # Save the model using the recommended.h5 file extension\n",
    "    model.save('chatbot_model.h5')\n",
    "    print('Model training complete and saved as \"chatbot_model.h5\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
